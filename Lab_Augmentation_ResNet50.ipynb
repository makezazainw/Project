{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab-Augmentation ResNet50.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "13505w90q7WOMEpxxMVUh6z5ZhlmKDviy",
      "authorship_tag": "ABX9TyOjx9BbPjIOxRJKasjFNmo7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/makezazainw/Project/blob/main/Lab_Augmentation_ResNet50.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mew3lwAMaXDH"
      },
      "source": [
        "### **ติดตั้ง Kaggle เเละ เรียกใช้ไฟล์ json เพื่อติดต่อใช้ dataset ใน kaggle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5m_qIXOabnO"
      },
      "source": [
        "# Install Kaggle API\n",
        "!pip install -q kaggle\n",
        "!pip install -q kaggle-cli"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_kkUe3valvy"
      },
      "source": [
        "# only for google colab\n",
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = \"thammatattantipitham\" \n",
        "os.environ['KAGGLE_KEY'] = \"3cf826ab97707228fb1602519992740b\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtsvYa8manW1"
      },
      "source": [
        "!kaggle datasets download -d thammatattantipitham/augmentationthaiherb --unzip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ihulub6paq8a"
      },
      "source": [
        "#สร้างไฟล์ folder ขึ้นมาเเละเพิ่มข้อมูลเข้าไปใน folder\n",
        "!mkdir augmentationlabthaiherb\n",
        "!mv  \"train\" \"val\" augmentationlabthaiherb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUU4GFLxa2pv"
      },
      "source": [
        "### **เริ่มต้นการ import library ต่างๆเเละการเรียกใช้ไฟล์**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08sp2eppa_-2"
      },
      "source": [
        "import sys\n",
        "import time\n",
        "import cv2\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import keras\n",
        "\n",
        "from skimage.transform import resize\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow.python.keras.utils import np_utils\n",
        "from tensorflow.python.keras.models import Sequential, Model\n",
        "from tensorflow.python.keras.layers import Dense, Dropout, Flatten, Activation, BatchNormalization\n",
        "from tensorflow.python.keras.layers.convolutional import Conv2D, MaxPooling2D, AveragePooling2D\n",
        "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.python.keras.optimizers import RMSprop\n",
        "from tensorflow.python.keras.applications.vgg19 import VGG19\n",
        "from tensorflow.python.keras.applications.mobilenet import MobileNet\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.applications import InceptionV3\n",
        "sys.version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SM_5RRq_bAAp"
      },
      "source": [
        "from tensorflow.compat.v1.keras.backend import set_session\n",
        "\n",
        "config = tf.compat.v1.ConfigProto()\n",
        "\n",
        "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
        "\n",
        "config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
        "\n",
        "sess = tf.compat.v1.Session(config=config)\n",
        "\n",
        "set_session(sess)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vm7Vog_bEAP"
      },
      "source": [
        "DATASET_PATH  = 'augmentationlabthaiherb/'\n",
        "IMAGE_SIZE    = (224, 224)\n",
        "NUM_CLASSES   = 10\n",
        "BATCH_SIZE    = 32  # try reducing batch size or freeze more layers if your GPU runs out of memory\n",
        "epochs    = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNu6CAPDbECl"
      },
      "source": [
        "#ทำ aug มาเเล้ว\n",
        "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDKsFAH_bEE1"
      },
      "source": [
        "#set training data\n",
        "train_generator = train_datagen.flow_from_directory(DATASET_PATH+ r\"train/\",\n",
        "                                                    target_size = IMAGE_SIZE,\n",
        "                                                    batch_size = BATCH_SIZE,\n",
        "                                                    class_mode='categorical',\n",
        "                                                    shuffle = True,\n",
        "                                                    subset='training')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1nB7VakbG_d"
      },
      "source": [
        "valid_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "valid_generator = valid_datagen.flow_from_directory(DATASET_PATH+ r\"val/\",                                      \n",
        "                                                    target_size = IMAGE_SIZE,\n",
        "                                                    batch_size = 1,\n",
        "                                                    class_mode='categorical',\n",
        "                                                    shuffle = True\n",
        "                                                    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5H1qvuqFbNM6"
      },
      "source": [
        "### **Model ResNet50**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9TaFF44bHBy"
      },
      "source": [
        "# Include the epoch in the file name (uses `str.format`)\n",
        "checkpoint_path = \"training_ResNet50/cp-{epoch:04d}.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "FREEZE_LAYERS = 2\n",
        "# Create a callback that saves the model's weights every 5 epochs\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_path, \n",
        "    verbose=1, \n",
        "    save_weights_only=True,\n",
        "    period=50)\n",
        "\n",
        "#Create the model\n",
        "base_model_resnet = ResNet50(include_top=False,weights='imagenet', input_tensor=None, input_shape=(IMAGE_SIZE[0],IMAGE_SIZE[1],3))\n",
        "x = base_model_resnet.output\n",
        "x = Flatten()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "output_layer = Dense(NUM_CLASSES, activation='softmax', name='softmax')(x)\n",
        "model_resnet = Model(inputs=base_model_resnet.input, outputs=output_layer)\n",
        "for layer in model_resnet.layers[:FREEZE_LAYERS]:\n",
        "    layer.trainable = False\n",
        "for layer in model_resnet.layers[FREEZE_LAYERS:]:\n",
        "    layer.trainable = True\n",
        "#Defining and Adding layers\n",
        "# model_resnet= Sequential()\n",
        "# #Add the Dense layers along with activation and batch normalization\n",
        "# model_resnet.add(base_model_resnet)\n",
        "# model_resnet.add(Flatten())\n",
        "\n",
        "# #Add the Dense layers along with activation and batch normalization\n",
        "# model_resnet.add(Dense(1024,activation=('relu'),input_dim=64))\n",
        "# model_resnet.add(Dense(512,activation=('relu'))) \n",
        "# model_resnet.add(Dropout(.4))\n",
        "# model_resnet.add(Dense(256,activation=('relu'))) \n",
        "# model_resnet.add(Dropout(.3))#Adding a dropout layer that will randomly drop 30% of the weights\n",
        "# model_resnet.add(Dense(128,activation=('relu')))\n",
        "# model_resnet.add(Dropout(.2))\n",
        "# model_resnet.add(Dense(NUM_CLASSES,activation=('softmax'))) #This is the classification layer\n",
        "\n",
        "# Save the weights using the `checkpoint_path` format\n",
        "model_resnet.save_weights(checkpoint_path.format(epoch=0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnKEGJjKbHEL"
      },
      "source": [
        "#Model summary\n",
        "model_resnet.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqpePAyfbHGf"
      },
      "source": [
        "model_resnet.compile(optimizer=Adam(lr=0.0001),  #Adam(lr=1e-5)\n",
        "                  loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHTNB_RRbHIX"
      },
      "source": [
        "#start to train the model\n",
        "start = time.time()\n",
        "history = model_resnet.fit_generator(generator=train_generator, epochs=epochs, steps_per_epoch=train_generator.samples/train_generator.batch_size, validation_data=valid_generator,validation_steps=valid_generator.samples/valid_generator.batch_size,callbacks=[cp_callback],verbose=1)\n",
        "\n",
        "end = time.time()\n",
        "duration = end - start\n",
        "print ('\\n model_ResNet50 took %0.2f seconds (%0.1f minutes) to train for %d epochs'%(duration, duration/60, epochs) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIrhE3LMcsBU"
      },
      "source": [
        "###**ขั้นตอนการ Save file Model เเละ Json**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jkhbODRb0RH"
      },
      "source": [
        "#เปลี่ยนชื่อไฟล์ทุกครั้งเวลา SAVE\n",
        "model_resnet.save('/content/drive/My Drive/Final Project/Dataset-Lab/Original/saved_model/model-resnet50(auglastest).h5') #ดูจำนวนรอบที่เทรนด้วยว่าทั้งหมดกี่รอบจริงๆเเล้ว\n",
        "model_resnet.save_weights('/content/drive/My Drive/Final Project/Dataset-Lab/Original/saved_model/weightaugmodel-resnet50.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUbahazNckQo"
      },
      "source": [
        "#Import dependencies\n",
        "!mkdir jsonfile\n",
        "import json\n",
        "from keras.models import model_from_json, load_model\n",
        "with open('jsonfile/model_resnet(aug).json', 'w') as f:\n",
        "    f.write(model_resnet.to_json())\n",
        "print(\"Saved model success\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctXhsc5pc-5N"
      },
      "source": [
        "###**เเสดงกราฟข้อมูลค่าความเเม่นยำเเละค่าความสูญเสีย Training เเละ Validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8UxPxbZdNzR"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = model_resnet.history.history['accuracy']\n",
        "val_acc = model_resnet.history.history['val_accuracy']\n",
        "\n",
        "loss= model_resnet.history.history['loss']\n",
        "val_loss= model_resnet.history.history['val_loss']\n",
        "\n",
        "\n",
        "plt.style.use(\"ggplot\")\n",
        "#  \"Accuracy\"\n",
        "plt.figure(figsize=(15, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epochs')\n",
        "#  \"Loss\"\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epochs')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0m3J0nidxqf"
      },
      "source": [
        "###**จุด Checkpoint**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjjRM_w0eE5D"
      },
      "source": [
        "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "latest"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBO8Q9Q4eE7E"
      },
      "source": [
        "# Load the previously saved weights\n",
        "model_resnet.load_weights(latest)\n",
        "\n",
        "# Re-evaluate the model\n",
        "loss, acc = model_resnet.evaluate(valid_generator, verbose=2)\n",
        "print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOnDw5WeeNXc"
      },
      "source": [
        "###**Save model อีกเเบบเพื่อนำไปเเปลงเป็น TensorflwLite**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCKmhkZ2eVSt"
      },
      "source": [
        "# Save the entire model as a SavedModel.\n",
        "model_resnet.save('saved_model/my_model') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiTau3PReVU1"
      },
      "source": [
        "model= tf.keras.models.load_model('saved_model/my_model')\n",
        "tflite_converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = tflite_converter.convert()\n",
        "open(\"tf_lite_model(200epochsresnet).tflite\", \"wb\").write(tflite_model) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7P6p0lzeq7g"
      },
      "source": [
        "###**ขั้นตอนของ Prediction เเบบบ Confusion Matrix**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tA579sRAeVXH"
      },
      "source": [
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "\n",
        "model = tf.keras.models.load_model('ใส่ path ที่ต้องการทำ .h5')\n",
        "filenames = valid_generator.filenames\n",
        "nb_samples = len(valid_generator)\n",
        "y_prob=[]\n",
        "y_act=[]\n",
        "valid_generator.reset()\n",
        "for _ in range(nb_samples):\n",
        "    x_test, y_test = valid_generator.next()\n",
        "    y_prob.append(model.predict(x_test))\n",
        "    y_act.append(y_test)\n",
        "    \n",
        "predicted_class = [list(train_generator.class_indices.keys())[i.argmax()] for i in y_prob]\n",
        "actual_class = [list(train_generator.class_indices.keys())[i.argmax()] for i in y_act]\n",
        "\n",
        "out_df = pd.DataFrame(np.vstack([predicted_class,actual_class]).T,columns=['predicted_class', 'actual_class'])\n",
        "confusion_matrix = pd.crosstab(out_df['actual_class'],out_df['predicted_class'], rownames=['Actual'], colnames=['Predicted'])\n",
        "\n",
        "sn.heatmap(confusion_matrix, cmap='Blues', annot= True, fmt='d')\n",
        "plt.show()\n",
        "print('test accuracy : {}'.format((np.diagonal(confusion_matrix).sum()/confusion_matrix.sum().sum()*100)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}