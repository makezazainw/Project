{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab-Original VGG19.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "13505w90q7WOMEpxxMVUh6z5ZhlmKDviy",
      "authorship_tag": "ABX9TyNcC5qta0Z9HF6vzI+wTPt8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/makezazainw/Project/blob/main/Lab_Original_VGG19.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mew3lwAMaXDH"
      },
      "source": [
        "### **ติดตั้ง Kaggle เเละ เรียกใช้ไฟล์ json เพื่อติดต่อใช้ dataset ใน kaggle**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5m_qIXOabnO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ceb8af57-30a3-456a-8223-0de8f0f665f1"
      },
      "source": [
        "# Install Kaggle API\n",
        "!pip install -q kaggle\n",
        "!pip install -q kaggle-cli"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 81kB 3.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 5.3MB 8.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 112kB 42.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 46.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 7.0MB/s \n",
            "\u001b[?25h  Building wheel for kaggle-cli (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for PrettyTable (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_kkUe3valvy"
      },
      "source": [
        "# only for google colab\n",
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = \"thammatattantipitham\" \n",
        "os.environ['KAGGLE_KEY'] = \"3cf826ab97707228fb1602519992740b\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtsvYa8manW1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f71424b0-c0b4-470b-bf9d-6713c6447221"
      },
      "source": [
        "!kaggle datasets download -d thammatattantipitham/originalthaiherb --unzip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading originalthaiherb.zip to /content\n",
            " 60% 9.00M/15.0M [00:01<00:01, 4.65MB/s]\n",
            "100% 15.0M/15.0M [00:01<00:00, 8.87MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ihulub6paq8a"
      },
      "source": [
        "#สร้างไฟล์ folder ขึ้นมาเเละเพิ่มข้อมูลเข้าไปใน folder\n",
        "!mkdir originalthaiherb\n",
        "!mv  \"train\" \"val\" originalthaiherb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUU4GFLxa2pv"
      },
      "source": [
        "### **เริ่มต้นการ import library ต่างๆเเละการเรียกใช้ไฟล์**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08sp2eppa_-2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "094438d3-4046-4756-cdd5-a42534abceca"
      },
      "source": [
        "import sys\n",
        "import time\n",
        "import cv2\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import keras\n",
        "\n",
        "from skimage.transform import resize\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow.python.keras.utils import np_utils\n",
        "from tensorflow.python.keras.models import Sequential, Model\n",
        "from tensorflow.python.keras.layers import Dense, Dropout, Flatten, Activation, BatchNormalization\n",
        "from tensorflow.python.keras.layers.convolutional import Conv2D, MaxPooling2D, AveragePooling2D\n",
        "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.python.keras.optimizers import RMSprop\n",
        "from tensorflow.python.keras.applications.vgg19 import VGG19\n",
        "from tensorflow.python.keras.applications.mobilenet import MobileNet\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.applications import InceptionV3\n",
        "sys.version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'3.6.9 (default, Oct  8 2020, 12:12:24) \\n[GCC 8.4.0]'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SM_5RRq_bAAp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28de294f-792b-42c0-9598-f0e20e382ed2"
      },
      "source": [
        "from tensorflow.compat.v1.keras.backend import set_session\n",
        "\n",
        "config = tf.compat.v1.ConfigProto()\n",
        "\n",
        "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
        "\n",
        "config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
        "\n",
        "sess = tf.compat.v1.Session(config=config)\n",
        "\n",
        "set_session(sess)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
            "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vm7Vog_bEAP"
      },
      "source": [
        "DATASET_PATH  = 'originalthaiherb/'\n",
        "IMAGE_SIZE    = (224, 224)\n",
        "NUM_CLASSES   = 10\n",
        "BATCH_SIZE    = 32  # try reducing batch size or freeze more layers if your GPU runs out of memory\n",
        "epochs    = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNu6CAPDbECl"
      },
      "source": [
        "#ทำ aug มาเเล้ว\n",
        "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDKsFAH_bEE1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10db58f7-0b2e-4a2d-c80a-519cc22c1f91"
      },
      "source": [
        "#set training data\n",
        "train_generator = train_datagen.flow_from_directory(DATASET_PATH+ r\"train/\",\n",
        "                                                    target_size = IMAGE_SIZE,\n",
        "                                                    # batch_size = BATCH_SIZE,\n",
        "                                                    class_mode='categorical',\n",
        "                                                    shuffle = True,\n",
        "                                                    subset='training')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1600 images belonging to 10 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1nB7VakbG_d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74f5f95b-f2a4-42e0-d646-e2a8a57a20e6"
      },
      "source": [
        "valid_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "valid_generator = valid_datagen.flow_from_directory(DATASET_PATH+ r\"val/\",                                      \n",
        "                                                    target_size = IMAGE_SIZE,\n",
        "                                                    # batch_size = 1,\n",
        "                                                    class_mode='categorical',\n",
        "                                                    shuffle = True\n",
        "                                                    )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 410 images belonging to 10 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5H1qvuqFbNM6"
      },
      "source": [
        "### **Model VGG19**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9TaFF44bHBy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58c7dd31-c8c7-4c84-a648-ee7e34180746"
      },
      "source": [
        "# Include the epoch in the file name (uses `str.format`)\n",
        "checkpoint_path = \"training_VGG19/cp-{epoch:04d}.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# Create a callback that saves the model's weights every 5 epochs\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_path, \n",
        "    verbose=1, \n",
        "    save_weights_only=True,\n",
        "    period=50)\n",
        "\n",
        "#Create the model\n",
        "def create_model_from_VGG19():\n",
        "    \"\"\"\n",
        "      use VGG19\n",
        "    \"\"\"\n",
        "    model = VGG19(weights = \"imagenet\", include_top=False, input_tensor=None, input_shape = (224, 224, 3))\n",
        "    \n",
        "    #5 layers เเรก จะไม่ถูกฝึกเนื่องจากใช้วิธีการ freeze ไว้เเล้ว\n",
        "    for layer in model.layers[:1]:\n",
        "      layer.trainable = False\n",
        "      \n",
        "    #ทำการเพิ่ม layers ที่กำหนดขึ้นมาเอง เป็นการปรับเเต่งเพื่อเพิ่มประสิทธิภาพ \n",
        "    x = model.output\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(1024, activation=\"relu\")(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(1024, activation=\"relu\")(x)\n",
        "    predictions = Dense(NUM_CLASSES, activation=\"softmax\")(x)\n",
        "    \n",
        "    #สร้าง model ชั้นสุดท้ายเพื่อจะนำมา compile\n",
        "    model_VGG19 = Model(inputs = model.input, outputs = predictions)\n",
        "    \n",
        "    model_VGG19.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.0001), metrics=['accuracy']) # optimizer=RMSprop(lr=0.001)\n",
        "\n",
        "    return model_VGG19\n",
        "\n",
        "    # Save the weights using the `checkpoint_path` format\n",
        "    model_VGG19.save_weights(checkpoint_path.format(epoch=0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnKEGJjKbHEL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c9fc036-88bf-4fcc-f20d-e12ab891c042"
      },
      "source": [
        "model_VGG19 = create_model_from_VGG19()\n",
        "model_VGG19.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "80142336/80134624 [==============================] - 1s 0us/step\n",
            "Model: \"functional_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv4 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv4 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv4 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 1024)              25691136  \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1024)              1049600   \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                10250     \n",
            "=================================================================\n",
            "Total params: 46,775,370\n",
            "Trainable params: 46,775,370\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqpePAyfbHGf"
      },
      "source": [
        "start = time.time()\n",
        "\n",
        "history = model_VGG19.fit_generator(\n",
        "    generator=train_generator, \n",
        "    steps_per_epoch=train_generator.samples//BATCH_SIZE,   # -> 106 # images 3392 = steps * batch_size = 106 * 32 \n",
        "    epochs=epochs, \n",
        "    validation_steps=valid_generator.samples//BATCH_SIZE, # -> 26 # images 832 = steps * batch_size = 26 * 32\n",
        "    validation_data=valid_generator,\n",
        "    callbacks = [cp_callback],\n",
        "    verbose = 1\n",
        ")\n",
        "\n",
        "end = time.time()\n",
        "duration = end - start\n",
        "print ('\\n model_VGG19 took %0.2f seconds (%0.1f minutes) to train for %d epochs'%(duration, duration/60, epochs) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIrhE3LMcsBU"
      },
      "source": [
        "###**ขั้นตอนการ Save file Model เเละ Json**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jkhbODRb0RH"
      },
      "source": [
        "#เปลี่ยนชื่อไฟล์ทุกครั้งเวลา SAVE\n",
        "model_VGG19.save('/content/drive/My Drive/Final Project/Dataset-Lab/Original/saved_model/model-vgg19(lastest).h5') #ดูจำนวนรอบที่เทรนด้วยว่าทั้งหมดกี่รอบจริงๆเเล้ว\n",
        "model_VGG19.save_weights('/content/drive/My Drive/Final Project/Dataset-Lab/Original/saved_model/weightmodel-vgg19.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUbahazNckQo"
      },
      "source": [
        "#Import dependencies\n",
        "!mkdir jsonfile\n",
        "import json\n",
        "from keras.models import model_from_json, load_model\n",
        "with open('jsonfile/model_VGG19.json', 'w') as f:\n",
        "    f.write(model_VGG19.to_json())\n",
        "print(\"Saved model success\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctXhsc5pc-5N"
      },
      "source": [
        "###**เเสดงกราฟข้อมูลค่าความเเม่นยำเเละค่าความสูญเสีย Training เเละ Validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8UxPxbZdNzR"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = model_VGG19.history.history['accuracy']\n",
        "val_acc = model_VGG19.history.history['val_accuracy']\n",
        "\n",
        "loss= model_VGG19.history.history['loss']\n",
        "val_loss= model_VGG19.history.history['val_loss']\n",
        "\n",
        "\n",
        "plt.style.use(\"ggplot\")\n",
        "#  \"Accuracy\"\n",
        "plt.figure(figsize=(15, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epochs')\n",
        "#  \"Loss\"\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epochs')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0m3J0nidxqf"
      },
      "source": [
        "###**จุด Checkpoint**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjjRM_w0eE5D"
      },
      "source": [
        "latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "latest"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBO8Q9Q4eE7E"
      },
      "source": [
        "# Load the previously saved weights\n",
        "model_VGG19.load_weights(latest)\n",
        "\n",
        "# Re-evaluate the model\n",
        "loss, acc = model_VGG19.evaluate(valid_generator, verbose=2)\n",
        "print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOnDw5WeeNXc"
      },
      "source": [
        "###**Save model อีกเเบบเพื่อนำไปเเปลงเป็น TensorflwLite**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCKmhkZ2eVSt"
      },
      "source": [
        "# Save the entire model as a SavedModel.\n",
        "model_VGG19.save('saved_model/my_model') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiTau3PReVU1"
      },
      "source": [
        "model= tf.keras.models.load_model('saved_model/my_model')\n",
        "tflite_converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = tflite_converter.convert()\n",
        "open(\"tf_lite_model(200epochsvgg19).tflite\", \"wb\").write(tflite_model) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7P6p0lzeq7g"
      },
      "source": [
        "###**ขั้นตอนของ Prediction เเบบบ Confusion Matrix**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tA579sRAeVXH"
      },
      "source": [
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "\n",
        "model = tf.keras.models.load_model('ใส่ path ที่ต้องการทำเป็นไฟล์ .h5')\n",
        "filenames = valid_generator.filenames\n",
        "nb_samples = len(valid_generator)\n",
        "y_prob=[]\n",
        "y_act=[]\n",
        "valid_generator.reset()\n",
        "for _ in range(nb_samples):\n",
        "    x_test, y_test = valid_generator.next()\n",
        "    y_prob.append(model.predict(x_test))\n",
        "    y_act.append(y_test)\n",
        "\n",
        "\n",
        "predicted_class = [list(train_generator.class_indices.keys())[i.argmax()] for i in y_prob]\n",
        "actual_class = [list(train_generator.class_indices.keys())[i.argmax()] for i in y_act]\n",
        "\n",
        "out_df = pd.DataFrame(np.vstack([predicted_class,actual_class]).T,columns=['predicted_class', 'actual_class'])\n",
        "confusion_matrix = pd.crosstab(out_df['actual_class'],out_df['predicted_class'], rownames=['Actual'], colnames=['Predicted'])\n",
        "\n",
        "sn.heatmap(confusion_matrix, cmap='Blues', annot= True, fmt='d')\n",
        "plt.show()\n",
        "print('test accuracy : {}'.format((np.diagonal(confusion_matrix).sum()/confusion_matrix.sum().sum()*100)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}